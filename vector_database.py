# -*- coding: utf-8 -*-
"""vector_database.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-PX43a99fG0fRQEix9728sewCb7BKIz4
"""

!pip install sentence-transformers faiss-cpu

import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

# 1. Load a pre-trained embedding model
# 'all-MiniLM-L6-v2' is lightweight, fast, and perfect for Colab
print("Loading model...")
model = SentenceTransformer('all-MiniLM-L6-v2')

# 2. Define a small dataset of text documents
documents = [
    "Variable Refrigerant Flow (VRF) systems provide highly efficient, zoned temperature control for modern commercial buildings.",
    "Centrifugal chillers are heavily utilized in large-scale industrial cooling and centralized air conditioning plants.",
    "Proper ductwork sizing is critical to minimize pressure drop, reduce airflow noise, and maintain system efficiency.",
    "Fire sprinkler systems and plumbing layouts must be strictly designed according to NFPA and local building codes.",
    "The Python programming language is widely used for machine learning and artificial intelligence tasks."
]

# 3. Convert documents into vector embeddings
print("Generating embeddings...")
embeddings = model.encode(documents)

# 4. Initialize the FAISS vector database
# We use an L2 (Euclidean distance) index for simplicity
dimension = embeddings.shape[1] # The size of the vector array
index = faiss.IndexFlatL2(dimension)

# Add our document embeddings to the FAISS index
index.add(embeddings)
print(f"Successfully indexed {index.ntotal} documents.\n")

# 5. Define a search function (Semantic Similarity + Top-K Retrieval)
def search_documents(query, k=2):
    # Convert the search query into an embedding using the exact same model
    query_embedding = model.encode([query])

    # Search the FAISS index for the 'k' closest vectors
    distances, indices = index.search(query_embedding, k)

    print(f"Search Query: '{query}'")
    print("-" * 50)

    # Display the results
    for i in range(k):
        doc_index = indices[0][i]
        distance = distances[0][i]
        print(f"Rank {i+1} (Distance: {distance:.4f}): {documents[doc_index]}")
    print("\n")

# 6. Test the system
# Notice how the query doesn't share exact words with the target document
search_documents("How do I cool down a massive factory?", k=2)
search_documents("What are the rules for installing water pipes to stop building fires?", k=2)